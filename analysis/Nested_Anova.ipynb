{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Anova\n",
    "\n",
    "*This was the nested anova which I implemented. However, it was not the correct analysis to make. Rather than throwing it away, I wanted to keep it for future reference.*\n",
    "\n",
    "*Here are the notes I made at the time:*\n",
    "\n",
    "Because we have 50-odd individuals, each of which carried out 50+ samples, I thought I could carry out a nested anova. Here, there are two factors - one is the experimental condition, the second is the individual. Compare with Steve McKillup's example on p 224. He has prawns and ponds. His factors are treatment (analogous to our condition) and pond (analogous to our individual). Compare also with Jerrold H. Zar, Chapter 15, p 307: He has these factors: Drug - the \"group\" (anal. to Steve's treatment or our condition) and Source - the \"subgroup\" (anal. to Steve's pond and our individual).\n",
    "\n",
    "Our condition is a \"fixed effects factor\". Our individual is a \"random effects factor\".\n",
    "\n",
    "***However, because we apply each condition to each individual, we actually have a \"two factor with replication\" design with one fixed factor (condition) and one random factor (individual)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the data, which should be available in Matlab v7 format:\n",
    "import scipy.io as sio\n",
    "mat_workspace = sio.loadmat('AllData/fnames.mat')\n",
    "# fnames is used throughout the rest of this notebook, so this section needs to be evaluated!\n",
    "fnames = mat_workspace['fnames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "def getfnameid (filename):\n",
    "    # idarr[1] is the ID, idarr[0] is the experimenter, idarr[3] is the dated filename.\n",
    "    idarr = filename.split('/')\n",
    "    return idarr[1]\n",
    "\n",
    "# From the condition string, return an index for the condition. 0 is \n",
    "# \"No Distractor trial\", 1 is \"Synchronous Distractor trail\", 2 is\n",
    "# \"Asynchronous Distractor trial\".\n",
    "def getcondition (condition_string):\n",
    "    condition_index = -1\n",
    "    if 'No Dist' in condition_string:\n",
    "        condition_index = 0\n",
    "    elif 'Synchro' in condition_string:\n",
    "        condition_index = 1\n",
    "    elif 'Asynchr' in condition_string:\n",
    "        condition_index = 2\n",
    "    return condition_index\n",
    "\n",
    "# A Single Factor ANOVA calculation for three datasets\n",
    "def group_anova(no_dist_latencies,sync_latencies,async_latencies):\n",
    "\n",
    "    all_latencies = np.concatenate((no_dist_latencies, sync_latencies, async_latencies))\n",
    "        \n",
    "    # Compute grand mean\n",
    "    grand_mean = all_latencies.mean()\n",
    "    #print 'Grand mean:',grand_mean,'(',all_latencies.var(ddof=1),') not ',all_latencies.var(ddof=0)\n",
    "        \n",
    "    # Compute within-group variance\n",
    "    tmp1 = all_latencies\n",
    "    np.power(tmp1, 2)\n",
    "    within_group_dof = all_latencies.size-3\n",
    "    within_group_variance = tmp1.sum()/within_group_dof\n",
    "    #print 'within_group_variance',within_group_variance\n",
    "    \n",
    "    nodist_mean = no_dist_latencies.mean()\n",
    "    sync_mean = sync_latencies.mean()\n",
    "    async_mean = async_latencies.mean()\n",
    "    \n",
    "    # Compute amoung-group variance\n",
    "    tmp1 = np.power (grand_mean - nodist_mean, 2)*no_dist_latencies.size\n",
    "    tmp2 = np.power (grand_mean - sync_mean, 2)*sync_latencies.size\n",
    "    tmp3 = np.power (grand_mean - async_mean, 2)*async_latencies.size\n",
    "    sosquares = tmp1 + tmp2 + tmp3\n",
    "    between_group_dof = 2 # 3 conditions => 3 groups, so 3-1 DOF\n",
    "    between_group_variance = sosquares / between_group_dof\n",
    "    #print 'between_group_variance',between_group_variance\n",
    "        \n",
    "    # Now compute the F ratio\n",
    "    F = between_group_variance/within_group_variance\n",
    "        \n",
    "    # Lastly, what's the probability for this?\n",
    "    P = 1-special.fdtr(between_group_dof,within_group_dof,F)\n",
    "        \n",
    "    return (F, between_group_dof, within_group_dof, P)\n",
    "\n",
    "# Take n sub-samples from distn\n",
    "def subsample (distn, n):\n",
    "    counter = 0\n",
    "    subsamp = []\n",
    "    for s in distn:\n",
    "        if np.random.uniform()>0.5:\n",
    "            subsamp = np.append(subsamp, s)\n",
    "            counter = counter + 1\n",
    "        if counter >= n:\n",
    "            break\n",
    "    return subsamp\n",
    "\n",
    "# Libs used in class individual\n",
    "from scipy import special\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Class for an individual's data.\n",
    "class individual:\n",
    "    def __init__(self, subj_id):\n",
    "        self.subj_id = subj_id;\n",
    "        self.no_dist_latencies = np.ndarray(1)\n",
    "        self.sync_latencies = np.ndarray(1)\n",
    "        self.async_latencies = np.ndarray(1)\n",
    "        self._nodist_mean = -1\n",
    "        self._sync_mean = -1\n",
    "        self._async_mean = -1\n",
    "        self._nodist_std = -1\n",
    "        self._sync_std = -1\n",
    "        self._async_std = -1\n",
    "\n",
    "    # Compute ANOVA for this individual\n",
    "    def anova(self):\n",
    "        F, between_group_dof, within_group_dof, P = group_anova (self.no_dist_latencies, self.sync_latencies, self.async_latencies)\n",
    "        return (F, between_group_dof, within_group_dof, P)\n",
    "\n",
    "    def reportmeans (self):\n",
    "        print \"Mean(SD): No distr: {0:.2f} ({1:.2f}) Sync: {2:.2f} ({3:.2f}) Async: {4:.2f} ({5:.2f})\".format(self.nodist_mean(), self.nodist_std(), self.sync_mean(), self.sync_std(), self.async_mean(), self.async_std())                \n",
    "\n",
    "    # Batch up all data in a form suitable for statsmodel's MultiComparison class. This means\n",
    "    # concatenating the ND, SD & AD data intoa single array, and making a \"label\" array to match.\n",
    "    def getMultiComparisonData (self):\n",
    "        d = np.hstack((self.no_dist_latencies,self.sync_latencies,self.async_latencies))\n",
    "\n",
    "        nd_labels = np.ndarray(shape=(self.no_dist_latencies.size,), dtype=object)\n",
    "        nd_labels.fill('ND')\n",
    "\n",
    "        sd_labels = np.ndarray(shape=(self.sync_latencies.size,), dtype=object)\n",
    "        sd_labels.fill('SD')\n",
    "\n",
    "        ad_labels = np.ndarray(shape=(self.async_latencies.size,), dtype=object)\n",
    "        ad_labels.fill('AD')\n",
    "\n",
    "        l = np.hstack((nd_labels,sd_labels,ad_labels))\n",
    "\n",
    "        # d is the data array, l is the label array.\n",
    "        return (d, l)\n",
    "\n",
    "    # Do a full set of graphs to show the normality of the data. Show QQ plots,\n",
    "    # histograms of the distributions and results of Shapiro-Wilks tests for comparison.\n",
    "    # Pass in the significance level for the S-W test.\n",
    "    def shownormality (self, alpha):\n",
    "        f, axarr = plt.subplots(3, 2)\n",
    "        \n",
    "        #ax1.set_title('QQ plots')\n",
    "        fig1 = sm.qqplot(self.no_dist_latencies, fit=True, line='45',ax=axarr[0,0])\n",
    "        fig2 = sm.qqplot(self.sync_latencies, fit=True, line='45',ax=axarr[1,0])\n",
    "        fig3 = sm.qqplot(self.async_latencies, fit=True, line='45', ax=axarr[2,0])\n",
    "        axarr[0,0].set_title('QQ Plots')\n",
    "        \n",
    "        W, p = stats.shapiro (subsample(self.no_dist_latencies, 25))\n",
    "        isNormal = False\n",
    "        if p > alpha:\n",
    "            isNormal = True\n",
    "        sw = 'ND. Mean/SD:{2:.2f}/{3:.2f} W={0:.2f}, p={1:.2f} (Normal:{4})'.format(W,p,self.nodist_mean(),self.nodist_std(),isNormal)\n",
    "        axarr[0,1].hist(self.no_dist_latencies, bins=20, label=sw)\n",
    "        axarr[0,1].legend(prop={'size':9})\n",
    "        axarr[0,1].set_title('Dist\\'ns with Shapiro-Wilks stats')\n",
    "        \n",
    "        W, p = stats.shapiro (subsample(self.sync_latencies, 25))\n",
    "        isNormal = False\n",
    "        if p > alpha:\n",
    "            isNormal = True\n",
    "        sw = 'SD. Mean/SD:{2:.2f}/{3:.2f} W={0:.2f}, p={1:.2f} (Normal:{4})'.format(W,p,self.sync_mean(),self.sync_std(),isNormal)\n",
    "        axarr[1,1].hist(self.sync_latencies, bins=20, label=sw)\n",
    "        axarr[1,1].legend(prop={'size':9})\n",
    "\n",
    "        W, p = stats.shapiro (subsample(self.async_latencies, 25))\n",
    "        isNormal = False\n",
    "        if p > alpha:\n",
    "            isNormal = True\n",
    "        sw = 'AD. Mean/SD:{2:.2f}/{3:.2f} W={0:.2f}, p={1:.2f} (Normal:{4})'.format(W,p,self.async_mean(),self.async_std(),isNormal)\n",
    "        axarr[2,1].hist(self.async_latencies, bins=20, label=sw)\n",
    "        axarr[2,1].legend(prop={'size':9})\n",
    "\n",
    "        # Fine-tune figure; make subplots close to each other and hide x ticks for\n",
    "        # all but bottom plot.\n",
    "        f.subplots_adjust(hspace=0)\n",
    "        plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    # Apply Shapiro-Wilk test. Null hypothesis is that the data are normally\n",
    "    # distributed. If p < alpha then null hypothesis must be rejected and data\n",
    "    # cannot be considered to be normally distributed.\n",
    "    def shapiroWilk (self, condition, alpha):\n",
    "\n",
    "        W = -1\n",
    "        p = -1\n",
    "        isNormal = False\n",
    "\n",
    "        if condition == 0:\n",
    "            W, p = stats.shapiro (subsample(self.no_dist_latencies, 25))\n",
    "        elif condition == 1:\n",
    "            W, p = stats.shapiro (subsample(self.sync_latencies, 25))\n",
    "        elif condition == 2:\n",
    "            W, p = stats.shapiro (subsample(self.async_latencies, 25))\n",
    "        # else leave W,p,isNormal with default values\n",
    "\n",
    "        if p > alpha:\n",
    "            isNormal = True\n",
    "\n",
    "        return W, p, isNormal\n",
    "    \n",
    "    # Do a Quantile-Quantile plot to compare against normal distribution\n",
    "    def qqplot (self):\n",
    "        f, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)\n",
    "        ax1.set_title('QQ plots')\n",
    "        fig1 = sm.qqplot(self.no_dist_latencies, fit=True, line='45',ax=ax1)\n",
    "        fig2 = sm.qqplot(self.sync_latencies, fit=True, line='45',ax=ax2)\n",
    "        fig3 = sm.qqplot(self.async_latencies, fit=True, line='45', ax=ax3)\n",
    "        # Fine-tune figure; make subplots close to each other and hide x ticks for\n",
    "        # all but bottom plot.\n",
    "        f.subplots_adjust(hspace=0)\n",
    "        plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)\n",
    "        plt.show()\n",
    "        return f\n",
    "\n",
    "    def excludeOutliers (self, num_sds):\n",
    "        #print \"Removing outliers\", num_sds, \"SDs away from the mean\"\n",
    "        r = 0\n",
    "        while r < self.no_dist_latencies.size:\n",
    "            upperlimit = self.nodist_mean() + self.nodist_std()*num_sds\n",
    "            lowerlimit = self.nodist_mean() - self.nodist_std()*num_sds\n",
    "            if self.no_dist_latencies[r] > upperlimit or self.no_dist_latencies[r] < lowerlimit:\n",
    "                # outlier, so delete\n",
    "                self.no_dist_latencies = np.delete (self.no_dist_latencies, r, 0)\n",
    "                r = r - 1\n",
    "            r = r + 1\n",
    "\n",
    "        r = 0\n",
    "        while r < self.sync_latencies.size:\n",
    "            if self.sync_latencies[r] > (self.sync_mean() + self.sync_std()*num_sds) or self.sync_latencies[r] < (self.sync_mean() - self.sync_std()*num_sds):\n",
    "                self.sync_latencies = np.delete (self.sync_latencies, r, 0)\n",
    "                r = r - 1\n",
    "            r = r + 1\n",
    "\n",
    "        r = 0\n",
    "        while r < self.async_latencies.size:\n",
    "            if self.async_latencies[r] > (self.async_mean() + self.async_std()*num_sds) or self.async_latencies[r] < (self.async_mean() - self.async_std()*num_sds):\n",
    "                self.async_latencies = np.delete (self.async_latencies, r, 0)\n",
    "                r = r - 1\n",
    "            r = r + 1\n",
    "\n",
    "    def randomly_subsample_data (self, num_data):\n",
    "        while len(self.no_dist_latencies) > num_data:\n",
    "            remove_this = randint (0,len(self.no_dist_latencies)-1)\n",
    "            self.no_dist_latencies = np.delete(self.no_dist_latencies, remove_this)\n",
    "        while len(self.sync_latencies) > num_data:\n",
    "            remove_this = randint (0,len(self.sync_latencies)-1)\n",
    "            self.sync_latencies = np.delete(self.sync_latencies, remove_this)\n",
    "        while len(self.async_latencies) > num_data:\n",
    "            remove_this = randint (0,len(self.async_latencies)-1)\n",
    "            self.async_latencies = np.delete(self.async_latencies, remove_this)\n",
    " \n",
    "    def graph1(self):\n",
    "        print 'Showing graph for ', ind.subj_id\n",
    "        means = (self.nodist_mean(), self.sync_mean(), self.async_mean())\n",
    "        stds = (self.nodist_std(), self.sync_std(), self.async_std())\n",
    "        index = np.arange(3)\n",
    "        opacity = 0.4\n",
    "        error_config = {'ecolor': '0.3'}\n",
    "        rects1 = plt.bar(index, means, 0.2,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 yerr=stds,\n",
    "                 error_kw=error_config,\n",
    "                 label=ind.subj_id)\n",
    "        # Now draw the points on a scatter graph\n",
    "        nodist_cond_x = np.zeros(self.no_dist_latencies.size)\n",
    "        sync_cond_x = np.ones(self.sync_latencies.size)\n",
    "        async_cond_x = 2*np.ones(self.async_latencies.size)\n",
    "        nodist_pts = plt.scatter(nodist_cond_x, self.no_dist_latencies)\n",
    "        sync_pts = plt.scatter(sync_cond_x, self.sync_latencies)\n",
    "        async_pts = plt.scatter(async_cond_x, self.async_latencies)\n",
    "        plt.xlabel('Condition 0:ND 1:S 2:AS')\n",
    "        plt.ylabel('Latency (ms)')\n",
    "        plt.title(self.subj_id)\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    # Compute the sum of the squared displacements from the mean for all three conditions\n",
    "    def sumofsquare_displacements_all_from_value(self, value):\n",
    "        sos = self.sumofsquare_displacements_from_value(0,value) + self.sumofsquare_displacements_from_value(1,value) + self.sumofsquare_displacements_from_value(2,value)\n",
    "        return sos\n",
    "\n",
    "    # Compute the sum of the squared displacements from the mean for all three conditions\n",
    "    def sumofsquare_displacements_all(self):\n",
    "        sos = self.sumofsquare_displacements(0) + self.sumofsquare_displacements(1) + self.sumofsquare_displacements(2)\n",
    "        return sos\n",
    "\n",
    "    # Compute the sum of the squared displacements from the mean for the given condition\n",
    "    def sumofsquare_displacements(self, condition):\n",
    "        if condition == 0:\n",
    "            mn = self.nodist_mean()\n",
    "            squares = np.power((self.no_dist_latencies - mn), 2)\n",
    "        elif condition == 1:\n",
    "            mn = self.sync_mean()\n",
    "            squares = np.power((self.sync_latencies - mn), 2)\n",
    "        else: # condition 2\n",
    "            mn = self.async_mean()\n",
    "            squares = np.power((self.async_latencies - mn), 2)\n",
    "        sos = np.sum(squares)\n",
    "        return sos\n",
    "    \n",
    "    # Compute the sum of the squared displacements from the mean for the given condition\n",
    "    def sumofsquare_displacements_from_value(self, condition, value):\n",
    "        if condition == 0:\n",
    "            squares = np.power((self.no_dist_latencies - value), 2)\n",
    "            # Verification of this method:\n",
    "            #squares_alt = 0\n",
    "            #for i in self.no_dist_latencies:\n",
    "            #    imv = i - value\n",
    "            #    squares_alt += imv*imv\n",
    "            #print 'nd sum of squares:',np.sum(squares),'squares_alt:',squares_alt\n",
    "        elif condition == 1:\n",
    "            squares = np.power((self.sync_latencies - value), 2)\n",
    "        else: # condition 2\n",
    "            squares = np.power((self.async_latencies - value), 2)\n",
    "        sos = np.sum(squares)\n",
    "        return sos\n",
    "\n",
    "    def num_replicates_all(self):\n",
    "        n = self.num_replicates(0) + self.num_replicates(1) + self.num_replicates(2)\n",
    "        return n\n",
    "\n",
    "    def num_replicates(self, condition):\n",
    "        if condition == 0:\n",
    "            return self.no_dist_latencies.size\n",
    "        elif condition == 1:\n",
    "            return self.sync_latencies.size\n",
    "        else: # condition 2\n",
    "            return self.async_latencies.size\n",
    "\n",
    "    def nodist_mean(self):\n",
    "        self._nodist_mean = self.no_dist_latencies.mean()\n",
    "        return self._nodist_mean\n",
    "        # This didn't work yet:\n",
    "        #print \"nodist_mean called self._nodist_mean==\",self._nodist_mean\n",
    "        #if self.no_dist_latencies.size == 1\n",
    "        #    return self._nodist_mean\n",
    "        #if self._nodist_mean == -1:\n",
    "        #    self._nodist_mean = self.no_dist_latencies.mean()\n",
    "        #return self._nodist_mean\n",
    "        \n",
    "    def sync_mean(self):\n",
    "        self._sync_mean = self.sync_latencies.mean()\n",
    "        return self._sync_mean\n",
    "\n",
    "    def async_mean(self):\n",
    "        self._async_mean = self.async_latencies.mean()\n",
    "        return self._async_mean\n",
    "\n",
    "    def overall_mean(self):\n",
    "        all_latencies = np.concatenate((self.no_dist_latencies, self.sync_latencies, self.async_latencies))\n",
    "        return all_latencies.mean()\n",
    "\n",
    "    def overall_std(self):\n",
    "        all_latencies = np.concatenate((self.no_dist_latencies, self.sync_latencies, self.async_latencies))\n",
    "        return all_latencies.std()\n",
    "    \n",
    "    def nodist_std(self):\n",
    "        self._nodist_std = self.no_dist_latencies.std()\n",
    "        return self._nodist_std\n",
    "        \n",
    "    def sync_std(self):\n",
    "        self._sync_std = self.sync_latencies.std()\n",
    "        return self._sync_std\n",
    "\n",
    "    def async_std(self):\n",
    "        self._async_std = self.async_latencies.std()\n",
    "        return self._async_std\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Data container for subject {0}\".format(self.subj_id)\n",
    "\n",
    "def readIndividuals():\n",
    "    individuals = dict()\n",
    "    # Extract the data from the raw format and collate it into individual\n",
    "    # data containers, one per subject.\n",
    "    for fname in zip(*fnames):\n",
    "        # I'll use the subject ID as a key into output data structures\n",
    "        subj_id = getfnameid(fname[0][0])\n",
    "\n",
    "        # condition index is for the no distractor/sync distractor/async distractor\n",
    "        condition_index = getcondition(fname[1][0,0][36][0])\n",
    "\n",
    "        # Need ONE individual object for each subj_id.\n",
    "        if subj_id not in individuals:\n",
    "            individuals[subj_id] = individual(subj_id)\n",
    "\n",
    "        latencies = fname[4] # Use table with python index 4 - \"no move error\", \"target event\".\n",
    "        \n",
    "        alldata = fname[2] # Contains all data, including move errors\n",
    "        \n",
    "        if condition_index == 0:\n",
    "            individuals[subj_id].no_dist_latencies = latencies[:,4];\n",
    "        elif condition_index == 1:\n",
    "            individuals[subj_id].sync_latencies = latencies[:,4];\n",
    "            # For sync, also read errors\n",
    "            nerrs = 0\n",
    "            ndistractors = 0\n",
    "            #if subj_id == 'JS':\n",
    "            #    print 'Sync'\n",
    "            for d in alldata:\n",
    "                #if subj_id == 'JS' and d[2] > 0.0:\n",
    "                #    print d\n",
    "                ndistractors += 1\n",
    "                if d[2] > 0.0:\n",
    "                    nerrs += 1\n",
    "            individuals[subj_id].n_errors_per_distractor_sync = (float(nerrs) / float(ndistractors))\n",
    "\n",
    "        elif condition_index == 2: \n",
    "            individuals[subj_id].async_latencies = latencies[:,4];\n",
    "            # For async, also read errors\n",
    "            nerrs = 0\n",
    "            ndistractors = 0\n",
    "            #if subj_id == 'JS':\n",
    "            #    print 'Async'\n",
    "            for d in alldata:\n",
    "                #if subj_id == 'JS' and d[2] > 0.0:\n",
    "                #    print d\n",
    "                if d[2] > 0.0: # Count all errors for async\n",
    "                    nerrs += 1\n",
    "                if d[1] < 1.0:\n",
    "                    ndistractors += 1\n",
    "            if subj_id == 'JS':\n",
    "                print 'Num movement errors:',nerrs,'Num distractor events:',ndistractors\n",
    "            individuals[subj_id].n_errors_per_distractor_async = (float(nerrs) / float(ndistractors))\n",
    "    \n",
    "    return individuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def nested_anova(individuals, nodist_latencies, sync_latencies, async_latencies):\n",
    "\n",
    "    all_latencies = np.concatenate((nodist_latencies, sync_latencies, async_latencies))\n",
    "        \n",
    "    # Compute/store grand mean and condition means:\n",
    "    grand_mean = all_latencies.mean()\n",
    "    nodist_mean = nodist_latencies.mean()\n",
    "    sync_mean = sync_latencies.mean()\n",
    "    async_mean = async_latencies.mean()\n",
    "\n",
    "    # First. Error estimate from displacements within sub-groups (individuals)\n",
    "    sos_error = 0\n",
    "    dof_error = 0\n",
    "    for ikey in individuals:\n",
    "        i = individuals[ikey]\n",
    "        sos_error = sos_error + i.sumofsquare_displacements_all()\n",
    "        dof_error = dof_error + (i.num_replicates_all() - 1)\n",
    "    meansquare_for_error = sos_error/dof_error\n",
    "    \n",
    "    # Second Ignore sub-groups (individuals) and compute meansquare for treatment, aka meansquare for group (condition) plus error.\n",
    "    sq_displacement_nodist = np.power(grand_mean - nodist_mean, 2);\n",
    "    sq_displacement_sync = np.power(grand_mean - sync_mean, 2);\n",
    "    sq_displacement_async = np.power(grand_mean - async_mean, 2);\n",
    "    dof_condition = 2\n",
    "    meansquare_for_condition_plus_ind_plus_err = (sq_displacement_nodist + sq_displacement_sync + sq_displacement_async)/dof_condition\n",
    "    \n",
    "    # Third Displacement of subgroups from  group (condition) means. meansquare for subgroups plus error \n",
    "    sq_displacement_nodist = 0\n",
    "    sq_displacement_sync = 0\n",
    "    sq_displacement_async = 0\n",
    "    for ikey in individuals:\n",
    "        i = individuals[ikey]\n",
    "        sq_displacement_nodist += np.power(nodist_mean-i.nodist_mean(), 2)\n",
    "        sq_displacement_sync += np.power(sync_mean-i.sync_mean(), 2)\n",
    "        sq_displacement_async += np.power(async_mean-i.async_mean(), 2)\n",
    "    dof_individual = len(individuals) - 1\n",
    "    meansquare_for_individuals_plus_err = (sq_displacement_nodist + sq_displacement_sync + sq_displacement_async)/dof_individual\n",
    "\n",
    "    # Now compute the F statistic - this is just the variance ratio:\n",
    "    # F for treatment aka condition\n",
    "    F_condition =  meansquare_for_condition_plus_ind_plus_err / meansquare_for_individuals_plus_err\n",
    "    # F for subgroup aka individual\n",
    "    F_individual = meansquare_for_individuals_plus_err / meansquare_for_error\n",
    "    \n",
    "    # Lastly, what's the probability for this?\n",
    "    P_condition = 1-special.fdtr(dof_condition, dof_individual, F_condition)\n",
    "    P_individual = 1-special.fdtr(dof_individual, dof_error, F_individual)\n",
    "    \n",
    "    return (F_condition, F_individual, dof_condition, dof_individual, dof_error, P_condition, P_individual)\n",
    "\n",
    "# Tukey testing? q = Condition mean A - Condition mean B / SEM where SEM = sqrt ( MS for individuals(conditions) / n )\n",
    "#\n",
    "# Implement examples from books to ensure that we get the same results.\n",
    "def nested_tukey(individuals, nodist_latencies, sync_latencies, async_latencies):\n",
    "\n",
    "    all_latencies = np.concatenate((nodist_latencies, sync_latencies, async_latencies))\n",
    "        \n",
    "    # Compute/store grand mean and condition means:\n",
    "    grand_mean = all_latencies.mean()\n",
    "    nodist_mean = nodist_latencies.mean()\n",
    "    sync_mean = sync_latencies.mean()\n",
    "    async_mean = async_latencies.mean()\n",
    "\n",
    "    # For a Tukey on nested/hierarchical data, ignore sub-groups (individuals) and compute meansquare for treatment, aka meansquare for group (condition) plus error.\n",
    "    sq_displacement_nodist = np.power(grand_mean - nodist_mean, 2);\n",
    "    sq_displacement_sync = np.power(grand_mean - sync_mean, 2);\n",
    "    sq_displacement_async = np.power(grand_mean - async_mean, 2);\n",
    "    dof_condition = 2\n",
    "    meansquare_for_condition_plus_ind_plus_err = (sq_displacement_nodist + sq_displacement_sync + sq_displacement_async)/dof_condition\n",
    "\n",
    "    #sos_error = 0\n",
    "    dof_error = 0\n",
    "    for ikey in individuals:\n",
    "        i = individuals[ikey]\n",
    "        #sos_error = sos_error + i.sumofsquare_displacements_all()\n",
    "        dof_error = dof_error + (i.num_replicates_all() - 1)\n",
    "    #meansquare_for_error = sos_error/dof_error\n",
    "    \n",
    "    # se for standard error? se chosen to match Zar p 229\n",
    "    se = np.sqrt(meansquare_for_condition_plus_ind_plus_err)\n",
    "    \n",
    "    # ND to SD Tukey\n",
    "    tukey1 = np.abs(nodist_mean - sync_mean) / se\n",
    "    tukey2 = np.abs(nodist_mean - async_mean) / se\n",
    "    tukey3 = np.abs(sync_mean - async_mean) / se\n",
    "    \n",
    "    # What's q_alpha_v_k? alpha = 0.05; v= error degrees of freedom (dof_error); k=3 (k means)\n",
    "    # From the book, that's q_0.05_inf_3, and it's 0.9539 X 1.588\n",
    "    q_alpha_v_k = 1.588\n",
    "    \n",
    "    print 'ND vs SD:',tukey1,'Significant?',(tukey1>q_alpha_v_k)\n",
    "    print 'ND vs AD:',tukey2,'Significant?',(tukey2>q_alpha_v_k)\n",
    "    print 'SD vs AD:',tukey3,'Significant?',(tukey3>q_alpha_v_k)\n",
    "    \n",
    "    # if q > q_0.05_inf_3 then the null hypothesis that the distributions are the same is rejected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num movement errors: 29 Num distractor events: 44\n",
      "Individual variances:\n",
      "F(54,8082)=0.81370802512 P=0.833468414456\n",
      "Condition variances:\n",
      "F(2,54)=0.294519057518 P=0.746078601439\n",
      "\n",
      "Tukey testing:\n",
      "ND vs SD: 1.68582931963 Significant? True\n",
      "ND vs AD: 1.76583679389 Significant? True\n",
      "SD vs AD: 0.0800074742587 Significant? False\n"
     ]
    }
   ],
   "source": [
    "# Read the fname data from the octave analysis into an array of individuals\n",
    "individuals = readIndividuals()\n",
    "\n",
    "# Three containers for every event latency\n",
    "nodist_latencies = [];\n",
    "sync_latencies = [];\n",
    "async_latencies = [];\n",
    "\n",
    "# Populate the three containers:\n",
    "for i in individuals:\n",
    "    ind = individuals[i]\n",
    "    # Exclude outlier latencies on a per-individual basis:\n",
    "    ind.excludeOutliers(2)\n",
    "    nodist_latencies = np.concatenate((nodist_latencies, ind.no_dist_latencies))\n",
    "    sync_latencies = np.concatenate((sync_latencies, ind.sync_latencies))\n",
    "    async_latencies = np.concatenate((async_latencies, ind.async_latencies))\n",
    "\n",
    "F_condition, F_individual, dof_condition, dof_individual, dof_error, P_condition, P_individual = nested_anova(individuals, nodist_latencies, sync_latencies, async_latencies)\n",
    "print 'Individual variances:'\n",
    "print 'F({0},{1})={2} P={3}'.format(dof_individual,dof_error,F_individual,P_individual)\n",
    "print 'Condition variances:'\n",
    "print 'F({0},{1})={2} P={3}'.format(dof_condition,dof_individual,F_condition,P_condition)\n",
    "\n",
    "print '\\nTukey testing:'\n",
    "nested_tukey(individuals, nodist_latencies, sync_latencies, async_latencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the nested Anova\n",
    "\n",
    "The null hypothesis is that the variances were the result only of samples drawn from a single population showing random variation.\n",
    "\n",
    "The alternative hypothesis is that the variances were the result of the samples being drawn from separate populations.\n",
    "\n",
    "In the above, the P value is the probability of the null hypothesis holding.\n",
    "\n",
    "So, P_cond = 0.75 says that there is a 75% chance that the null hypothesis is true and the condition variances for ND, sync and async samples were drawn from a single population.\n",
    "\n",
    "P_ind = 0.996 says there is a 99.6 % probability that the individual variances were drawn from a single population.\n",
    "\n",
    "**So this seems to be saying that there's no significant effect of the ND, sync or async conditions.**\n",
    "\n",
    "In fact, I realised that this is not the correct analysis to make, because each condition is carried out by each individual. The students were right to select a One way repeated measures Anova (see below in this document)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
